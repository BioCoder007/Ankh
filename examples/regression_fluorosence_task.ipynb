{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6aac13",
   "metadata": {},
   "source": [
    "### Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 7\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ankh\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from datasets import load_dataset\n",
    "import transformers.models.convbert as c_bert\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838401df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916de6b1",
   "metadata": {},
   "source": [
    "### Select the available device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Available device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079738e9",
   "metadata": {},
   "source": [
    "### Load Ankh large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = ankh.load_large_model()\n",
    "model.eval()\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb329135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b01ed",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ElnaggarLab/fluorosence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences, training_labels = dataset['train']['primary'], dataset['train']['log_fluorescence']\n",
    "validation_sequences, validation_labels = dataset['validation']['primary'], dataset['validation']['log_fluorescence']\n",
    "test_sequences, test_labels = dataset['test']['primary'], dataset['test']['log_fluorescence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b458dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of the labels to initialize \n",
    "# the final layer's bias with it for faster convergence in regression tasks.\n",
    "training_labels_mean = np.mean(dataset['train']['log_fluorescence'])\n",
    "training_labels_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(sequences, labels, max_length=None):\n",
    "    '''\n",
    "        Args:\n",
    "            sequences: list, the list which contains the protein primary sequences.\n",
    "            max_length, Integer, the maximum sequence length, \n",
    "            if there is a sequence that is larger than the specified sequence length will be post-truncated. \n",
    "    '''\n",
    "    if max_length is None:\n",
    "        max_length = len(max(training_sequences, key=lambda x: len(x)))\n",
    "    splitted_sequences = [list(seq[:max_length]) for seq in sequences]\n",
    "    return splitted_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cea65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_dataset(model, sequences, shift_left = 0, shift_right = -1):\n",
    "    inputs_embedding = []\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(sequences):\n",
    "            ids = tokenizer.batch_encode_plus([sample], add_special_tokens=True, \n",
    "                                              padding=True, is_split_into_words=True, \n",
    "                                              return_tensors=\"pt\")\n",
    "            embedding = model(input_ids=ids['input_ids'].to(device))[0]\n",
    "            embedding = embedding[0].detach().cpu().numpy()[shift_left:shift_right]\n",
    "            inputs_embedding.append(embedding)\n",
    "    return inputs_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb692b1b",
   "metadata": {},
   "source": [
    "### Preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences, training_labels = load_dataset(training_sequences, training_labels)\n",
    "validation_sequences, validation_labels = load_dataset(validation_sequences, validation_labels)\n",
    "test_sequences, test_labels = load_dataset(test_sequences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c15c7",
   "metadata": {},
   "source": [
    "### Extract sequences embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89efc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = embed_dataset(model, training_sequences)\n",
    "validation_embeddings = embed_dataset(model, validation_sequences)\n",
    "test_embeddings = embed_dataset(model, test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13914f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluorescenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'embed': torch.tensor(sample),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32).unsqueeze(-1)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = FluorescenceDataset(training_embeddings, training_labels)\n",
    "validation_dataset = FluorescenceDataset(validation_embeddings, validation_labels)\n",
    "test_ds = FluorescenceDataset(test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297c663",
   "metadata": {},
   "source": [
    "### Model initialization function for HuggingFace's trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(embed_dim, training_labels_mean=None):\n",
    "    hidden_dim = int(embed_dim / 2)\n",
    "    num_hidden_layers = 1\n",
    "    nlayers = 1\n",
    "    nhead = 4\n",
    "    dropout = 0.2\n",
    "    conv_kernel_size = 7\n",
    "    pooling = 'max' # available pooling methods ['avg', 'max']\n",
    "    downstream_model = ankh.ConvBertForRegression(input_dim=embed_dim, \n",
    "                                                  nhead=nhead, \n",
    "                                                  hidden_dim=hidden_dim, \n",
    "                                                  num_hidden_layers=num_hidden_layers, \n",
    "                                                  num_layers=nlayers, \n",
    "                                                  kernel_size=conv_kernel_size,\n",
    "                                                  dropout=dropout, \n",
    "                                                  pooling=pooling, \n",
    "                                                  training_labels_mean=training_labels_mean)\n",
    "    return downstream_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbc2c0",
   "metadata": {},
   "source": [
    "### Function for computing metrics, Spearman correlation is used in this regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    return {\n",
    "        \"spearmanr\": stats.spearmanr(p.label_ids, p.predictions).correlation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281f193",
   "metadata": {},
   "source": [
    "### Create and configure HuggingFace's TrainingArguments instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ankh_large'\n",
    "experiment = f'flu_{model_type}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results_{experiment}',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=1000,\n",
    "    learning_rate=1e-03,\n",
    "    weight_decay=0.0,\n",
    "    logging_dir=f'./logs_{experiment}',\n",
    "    logging_steps=200,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=False,\n",
    "    fp16_opt_level=\"02\",\n",
    "    run_name=experiment,\n",
    "    seed=seed,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_spearmanr\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c7d82",
   "metadata": {},
   "source": [
    "### Create HuggingFace Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed_dim = 1536\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=partial(model_init, embed_dim=model_embed_dim, training_labels_mean=training_labels_mean),\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983f0e8",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521367e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c904201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
